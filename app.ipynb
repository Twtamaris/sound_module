{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.62672180e+02 -6.29520630e+02 -3.89662781e+02 -3.14668488e+02\n",
      "  -2.89507355e+02]\n",
      " [ 0.00000000e+00  5.48909492e+01  2.99132233e+01  4.06137390e+01\n",
      "   8.44670563e+01]\n",
      " [ 0.00000000e+00 -5.14149017e+01 -4.34424591e+01 -2.81129761e+01\n",
      "  -1.47698698e+01]\n",
      " [ 0.00000000e+00 -4.58642244e+00  1.18799162e+00  9.45555401e+00\n",
      "  -4.95697290e-01]\n",
      " [ 0.00000000e+00 -3.93269615e+01 -4.49370956e+01 -3.73889084e+01\n",
      "  -4.23351860e+01]\n",
      " [ 0.00000000e+00  6.43753099e+00  5.65182304e+00  7.29870462e+00\n",
      "   1.72041166e+00]\n",
      " [ 0.00000000e+00  4.85045147e+00  3.33478403e+00  3.69341493e+00\n",
      "   6.71556807e+00]\n",
      " [ 0.00000000e+00 -1.73491669e+00 -5.17387247e+00 -4.79320526e+00\n",
      "   1.21403193e+00]\n",
      " [ 0.00000000e+00  1.36123390e+01  2.99189448e-01 -8.77024651e+00\n",
      "  -1.14590988e+01]\n",
      " [ 0.00000000e+00  1.36239994e+00  3.19477749e+00 -6.24655819e+00\n",
      "  -1.56038942e+01]\n",
      " [ 0.00000000e+00 -7.03666973e+00 -1.16040325e+01 -1.83685265e+01\n",
      "  -2.02493973e+01]\n",
      " [ 0.00000000e+00 -3.70566678e+00  7.55454600e-02 -5.53397942e+00\n",
      "  -9.05884457e+00]\n",
      " [ 0.00000000e+00 -2.24396477e+01 -1.99098511e+01 -1.91125641e+01\n",
      "  -1.29495850e+01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-8.62672180e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "       [-6.29520630e+02,  5.48909492e+01, -5.14149017e+01,\n",
       "        -4.58642244e+00, -3.93269615e+01,  6.43753099e+00,\n",
       "         4.85045147e+00, -1.73491669e+00,  1.36123390e+01,\n",
       "         1.36239994e+00, -7.03666973e+00, -3.70566678e+00,\n",
       "        -2.24396477e+01],\n",
       "       [-3.89662781e+02,  2.99132233e+01, -4.34424591e+01,\n",
       "         1.18799162e+00, -4.49370956e+01,  5.65182304e+00,\n",
       "         3.33478403e+00, -5.17387247e+00,  2.99189448e-01,\n",
       "         3.19477749e+00, -1.16040325e+01,  7.55454600e-02,\n",
       "        -1.99098511e+01],\n",
       "       [-3.14668488e+02,  4.06137390e+01, -2.81129761e+01,\n",
       "         9.45555401e+00, -3.73889084e+01,  7.29870462e+00,\n",
       "         3.69341493e+00, -4.79320526e+00, -8.77024651e+00,\n",
       "        -6.24655819e+00, -1.83685265e+01, -5.53397942e+00,\n",
       "        -1.91125641e+01],\n",
       "       [-2.89507355e+02,  8.44670563e+01, -1.47698698e+01,\n",
       "        -4.95697290e-01, -4.23351860e+01,  1.72041166e+00,\n",
       "         6.71556807e+00,  1.21403193e+00, -1.14590988e+01,\n",
       "        -1.56038942e+01, -2.02493973e+01, -9.05884457e+00,\n",
       "        -1.29495850e+01]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'sound_folder/clip_0.wav' \n",
    "audio, _ = librosa.load(file_path, sr=22050, duration=0.1)\n",
    "mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "print(mfccs)\n",
    "mfccs.shape\n",
    "mfccs.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract MFCC features from audio files\n",
    "def extract_features(file_path):\n",
    "    audio, _ = librosa.load(file_path, sr=22050, duration=0.1)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)\n",
    "    return mfccs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the dataset\n",
    "def prepare_dataset(sound_folder, no_sound_folder):\n",
    "    sound_files = os.listdir(sound_folder)\n",
    "    no_sound_files = os.listdir(no_sound_folder)\n",
    "\n",
    "    sound_data = []\n",
    "    no_sound_data = []\n",
    "    labels = []\n",
    "\n",
    "    for file in sound_files:\n",
    "        features = extract_features(os.path.join(sound_folder, file))\n",
    "        sound_data.append(features.T)\n",
    "        labels.append('sound')\n",
    "\n",
    "    for file in no_sound_files:\n",
    "        features = extract_features(os.path.join(no_sound_folder, file))\n",
    "        no_sound_data.append(features.T)\n",
    "        labels.append('no_sound')\n",
    "\n",
    "    X = np.vstack((sound_data, no_sound_data))\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = prepare_dataset('sound_folder', 'nosound_folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((896, 5, 13), (224, 5, 13), (896,), (224,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the MFCC features to fit the CNN input shape\n",
    "X_train_cnn = X_train[..., np.newaxis]\n",
    "X_test_cnn = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((896, 5, 13, 1), (896, 5, 13))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=X_train_cnn.shape[1:]),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # binary classification (sound or no sound)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28/28 [==============================] - 2s 20ms/step - loss: 1.6843 - accuracy: 0.8326 - val_loss: 0.0619 - val_accuracy: 0.9911\n",
      "Epoch 2/10\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.1173 - accuracy: 0.9855 - val_loss: 0.0096 - val_accuracy: 0.9955\n",
      "Epoch 3/10\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0320 - accuracy: 0.9933 - val_loss: 0.0041 - val_accuracy: 0.9955\n",
      "Epoch 4/10\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.0202 - accuracy: 0.9922 - val_loss: 0.0227 - val_accuracy: 0.9955\n",
      "Epoch 5/10\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0327 - accuracy: 0.9933 - val_loss: 0.0082 - val_accuracy: 0.9955\n",
      "Epoch 6/10\n",
      "28/28 [==============================] - 0s 6ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.0106 - val_accuracy: 0.9955\n",
      "Epoch 7/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.9944 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9978 - val_loss: 1.6875e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 0.9978 - val_loss: 1.6842e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 8.8453e-04 - accuracy: 1.0000 - val_loss: 1.6509e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f1f8da08b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sound_detection_module.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('sound_detection_module.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\baral\\AppData\\Local\\Temp\\tmp7eoi46ml\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\baral\\AppData\\Local\\Temp\\tmp7eoi46ml\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model to TensorFlow Lite format\n",
    "# If want to use in the mobile or low end device which do not contain tensorflow convert it to the TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "with open('sound_detection_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
